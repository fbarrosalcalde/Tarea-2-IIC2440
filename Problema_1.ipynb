{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea 2"
      ],
      "metadata": {
        "id": "0uZDgWk-m4TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 1"
      ],
      "metadata": {
        "id": "vUtJJS49m9Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hbWIPBj_M5h",
        "outputId": "b2bc3650-1d05-4796-bcbf-1eed63ca5e29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=01b4cb331a0f04c4d9c9d1ccbd8af851bb0f0bad10d7b72b39b62359ff2b3b91\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27eKJg3F_FMv",
        "outputId": "16e6a658-fb2c-436d-a5c8-d74017744753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "(2, 0.4625)\n",
            "(3, 0.14375)\n",
            "(4, 0.14375)\n",
            "Iteration 2:\n",
            "(3, 0.2340625)\n",
            "(4, 0.2340625)\n",
            "(2, 0.15968749999999998)\n",
            "Iteration 3:\n",
            "(2, 0.23645312500000001)\n",
            "(3, 0.1053671875)\n",
            "(4, 0.1053671875)\n",
            "Iteration 4:\n",
            "(3, 0.13799257812500001)\n",
            "(4, 0.13799257812500001)\n",
            "(2, 0.127062109375)\n",
            "Iteration 5:\n",
            "(2, 0.15479369140625002)\n",
            "(3, 0.09150139648437501)\n",
            "(4, 0.09150139648437501)\n",
            "Iteration 6:\n",
            "(3, 0.10328731884765627)\n",
            "(4, 0.10328731884765627)\n",
            "(2, 0.11527618701171877)\n",
            "Iteration 7:\n",
            "(2, 0.12529422102050783)\n",
            "(3, 0.08649237947998048)\n",
            "(4, 0.08649237947998048)\n",
            "Iteration 8:\n",
            "(3, 0.09075004393371583)\n",
            "(4, 0.09075004393371583)\n",
            "(2, 0.11101852255798342)\n",
            "Iteration 9:\n",
            "(2, 0.11463753734365846)\n",
            "(3, 0.08468287208714295)\n",
            "(4, 0.08468287208714295)\n",
            "Iteration 10:\n",
            "(3, 0.08622095337105484)\n",
            "(4, 0.08622095337105484)\n",
            "(2, 0.10948044127407151)\n",
            "Iteration 11:\n",
            "(2, 0.11078781036539662)\n",
            "(3, 0.0840291875414804)\n",
            "(4, 0.0840291875414804)\n",
            "Iteration 12:\n",
            "(3, 0.08458481940529357)\n",
            "(4, 0.08458481940529357)\n",
            "(2, 0.10892480941025834)\n",
            "Iteration 13:\n",
            "(2, 0.10939709649449954)\n",
            "(3, 0.0837930439993598)\n",
            "(4, 0.0837930439993598)\n",
            "Iteration 14:\n",
            "(3, 0.08399376601016231)\n",
            "(4, 0.08399376601016231)\n",
            "(2, 0.10872408739945584)\n",
            "Iteration 15:\n",
            "(2, 0.10889470110863797)\n",
            "(3, 0.08370773714476873)\n",
            "(4, 0.08370773714476873)\n",
            "Iteration 16:\n",
            "(3, 0.08378024797117115)\n",
            "(4, 0.08378024797117115)\n",
            "(2, 0.10865157657305342)\n",
            "Iteration 17:\n",
            "(2, 0.10871321077549548)\n",
            "(3, 0.08367692004354771)\n",
            "(4, 0.08367692004354771)\n",
            "Iteration 18:\n",
            "(3, 0.08370311457958558)\n",
            "(4, 0.08370311457958558)\n",
            "(2, 0.10862538203701555)\n",
            "Iteration 19:\n",
            "(2, 0.10864764739264775)\n",
            "(3, 0.0836657873657316)\n",
            "(4, 0.0836657873657316)\n",
            "Final Page Ranks:\n",
            "(2, 0.10864764739264775)\n",
            "(3, 0.0836657873657316)\n",
            "(4, 0.0836657873657316)\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Paso 1: Preparar RDD con nodos y Page Ranks iniciales\n",
        "sc = SparkContext(\"local\", \"PageRank\")\n",
        "nodes = [1, 2, 3, 4]\n",
        "num_nodes = len(nodes)\n",
        "initial_page_rank = 1.0 / num_nodes\n",
        "node_ranks = sc.parallelize([(node, initial_page_rank) for node in nodes])\n",
        "\n",
        "# Paso 2: Crear diccionario de vecinos para cada nodo\n",
        "edges = [(1, 2), (2, 3), (2, 4), (3, 2)]\n",
        "node_neighbors = sc.parallelize(edges).groupByKey().collectAsMap()\n",
        "\n",
        "# Función para preparar los mensajes que cada nodo enviará\n",
        "def prepare_messages(node_rank):\n",
        "    node, rank = node_rank\n",
        "    neighbors = node_neighbors.get(node, [])\n",
        "    num_neighbors = len(neighbors)\n",
        "    if num_neighbors == 0:\n",
        "        return []\n",
        "    rank_per_neighbor = rank / num_neighbors\n",
        "    return [(neighbor, rank_per_neighbor) for neighbor in neighbors]\n",
        "\n",
        "# Función para realizar el intercambio de mensajes y merge de los recibidos\n",
        "def exchange_messages(node_ranks):\n",
        "    messages = node_ranks.flatMap(prepare_messages)\n",
        "    node_ranks = messages.reduceByKey(lambda x, y: x + y)\n",
        "    return node_ranks\n",
        "\n",
        "# Función para actualizar el valor de PageRank para cada nodo considerando el damping factor\n",
        "def update_page_rank(node_rank):\n",
        "    node, rank = node_rank\n",
        "    new_rank = (rank * damping_factor) + ((1 - damping_factor) / num_nodes)\n",
        "    return (node, new_rank)\n",
        "\n",
        "# Parámetros de PageRank\n",
        "damping_factor = 0.85  # Factor de amortiguación (d)\n",
        "max_iterations = 25  # Número máximo de iteraciones\n",
        "min_diff = 0.0001  # Diferencia mínima entre iteraciones para detenerse\n",
        "\n",
        "# Paso 3 y 4: Iterar intercambio de mensajes, merge y actualización del valor de PageRank\n",
        "for iteration in range(max_iterations):\n",
        "    # Intercambio de mensajes y merge\n",
        "    node_ranks = exchange_messages(node_ranks)\n",
        "\n",
        "    # Actualización del valor de PageRank de cada nodo\n",
        "    node_ranks = node_ranks.map(update_page_rank)\n",
        "\n",
        "    # Mostrar los mensajes recibidos en cada iteración\n",
        "    print(f\"Iteration {iteration + 1}:\")\n",
        "    for node_rank in node_ranks.collect():\n",
        "        print(node_rank)\n",
        "\n",
        "    # Verificar la diferencia entre iteraciones\n",
        "    if iteration > 0:\n",
        "        diff = node_ranks.join(prev_node_ranks).map(lambda x: abs(x[1][0] - x[1][1])).sum()\n",
        "        if diff < min_diff:\n",
        "            break\n",
        "\n",
        "    # Guardar el valor de PageRank para la siguiente iteración\n",
        "    prev_node_ranks = node_ranks\n",
        "\n",
        "# Mostrar los nodos y sus valores finales de Page Rank\n",
        "print(\"Final Page Ranks:\")\n",
        "for node_rank in node_ranks.collect():\n",
        "    print(node_rank)"
      ]
    }
  ]
}